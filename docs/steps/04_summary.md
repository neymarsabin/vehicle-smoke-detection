Deep Learning for Smoky Vehicle Detection in Videos: A Practical Implementation GuideI. Introduction to Smoky Vehicle DetectionA. The Environmental and Technical ImperativeThe detection of smoky vehicles from video footage represents a significant advancement in environmental monitoring and traffic management. Traditionally, identifying high-emission vehicles has relied on expensive remote sensing methods employing ultraviolet-infrared light devices, typically utilized by environmental protection agencies.1 However, the advent of deep learning offers a superior and more cost-effective alternative, leveraging existing roadway Closed-Circuit Television (CCTV) cameras.3 This shift is driven by a growing global concern over air pollution stemming from vehicle exhausts, particularly black smoke emissions, which necessitate effective and continuous detection mechanisms.3A system capable of automatically identifying smoky vehicles in real-time can contribute significantly to environmental protection by enabling proactive pollution control and enforcement. Beyond environmental benefits, such a system can enhance traffic management by identifying vehicles that may impede visibility or cause accidents due to excessive smoke. Furthermore, it could potentially improve road safety by flagging vehicles with mechanical issues indicated by exhaust smoke.4 The integration of this technical solution with environmental policy creates a powerful synergy. By providing scalable, non-invasive, and continuous monitoring, deep learning models can generate actionable data that directly informs and enforces environmental regulations. This transforms reactive, periodic measurements into a proactive, real-time approach to emission control, thereby maximizing the real-world impact on public health and environmental quality.B. Why Deep Learning is Essential for Video AnalysisDeep learning, particularly through the application of Convolutional Neural Networks (CNNs), has fundamentally transformed the field of computer vision, yielding remarkable achievements in tasks such as object detection and semantic segmentation.5 For the intricate task of video analysis, deep learning algorithms are indispensable for the real-time identification and tracking of objects of interest within dynamic video sequences.7 These advanced capabilities enable the intelligent analysis of visual data, effectively converting conventional surveillance systems into automated, smart platforms capable of learning from visual inputs and making informed decisions.9Traditional image-based methods for smoke detection commonly rely on handcrafted features, such as specific colors, textures, or shapes.10 While these methods can be effective in controlled environments, their accuracy significantly diminishes when environmental conditions change or when encountering novel scenarios.10 Deep learning models, conversely, possess the inherent ability to automatically extract hierarchical "depth features" directly from raw data. This allows them to be far more adaptable to variable environments and dynamic phenomena like smoke and fire, demonstrating substantial performance improvements over conventional techniques.10 The capacity of CNNs, often combined with Recurrent Neural Networks (RNNs), to integrate temporal dynamics is pivotal for the rapid and precise identification of transient events, making deep learning an essential tool for robust video-based smoke detection.8 This represents a fundamental shift from human experts meticulously designing features for specific conditions to models learning these representations directly from vast datasets. The result is a more generalized and robust system, which is crucial for deployment in unpredictable real-world environments.C. Key Challenges in Smoke DetectionDetecting smoke from visual scenes is an inherently complex task due to the highly variable nature of smoke itself. Smoke exhibits significant variations in its shape, color, texture, and density, making it difficult to define a consistent visual signature.1 This inherent ambiguity often leads to confusion with other visually similar phenomena, such as shadows, clouds, haze, fog, steam, or dust, resulting in high false positive rates in detection systems.10The "transparent" and "amorphous" characteristics of smoke, particularly light or early-stage plumes, pose a unique challenge for pixel-level segmentation. Smoke lacks clear, defined textures and tends to blend seamlessly with the background, complicating its precise delineation.17 This difficulty is compounded when attempting to detect small smoke plumes, as conventional CNNs may struggle to focus on these subtle features, instead prioritizing more prominent background elements.10Furthermore, real-world deployment scenarios introduce additional complexities that degrade detection accuracy. These include highly variable lighting conditions (e.g., bright daylight, low-light nighttime, glare, backlighting), visual occlusions (e.g., other vehicles, environmental structures), and motion blur caused by fast-moving objects or camera shake.12 All these factors can significantly obscure the visual cues of smoke. Moreover, the scarcity of adequately annotated data specifically for smoky vehicles further hinders the generalization capabilities of deep learning models, as they require diverse and representative examples to learn effectively.1The challenge of smoke detection can be characterized as the "amorphous anomaly" problem. Smoke, as a transient phenomenon, lacks a fixed visual form, making it highly fluid and context-dependent. This fluidity directly contributes to a high false alarm rate because models may inadvertently learn "context biases," associating normal smoke-like phenomena (such as industrial chimney exhaust or steam) with actual anomalous vehicle emissions.15 The core difficulty is not merely detecting smoke, but rather performing discriminative detection—identifying features that are invariant and truly indicative of problematic vehicle smoke, distinguishing it from benign visual proxies. This necessitates robust feature learning and sophisticated loss functions capable of handling such subtle distinctions, along with strategies like Unbiased Multiple Instance Learning (UMIL) to mitigate context-dependent false positives.II. Core Deep Learning Concepts for Video AnalysisA. Object Detection ModelsObject detection algorithms are fundamental to identifying and localizing objects within images or video frames. These models are designed to predict both the class label of an object and the coordinates of its bounding box.51. Single-Stage Detectors (YOLO, SSD)Single-stage detectors, such as YOLO (You Only Look Once) and SSD (Single Shot MultiBox Detector), are characterized by their efficiency. They employ a single Convolutional Neural Network (CNN) to directly predict class labels and bounding box coordinates across the entire input image.5 YOLO, initially developed in 2015, operates by dividing the input image into a grid, with each grid cell responsible for predicting the presence and bounding box of objects. This architecture processes the entire image in a single forward pass, contributing significantly to its remarkable speed.5YOLO has gained widespread adoption in real-world applications due to its compelling balance of speed and accuracy, rendering it highly suitable for real-time scenarios such as video surveillance and autonomous driving.5 For instance, YOLOv8 represents a notable advancement in this lineage, offering impressive inference speeds, often exceeding 100 frames per second (FPS) in live scenarios, without compromising detection accuracy.26 The model effectively handles objects of varying scales through the use of anchor boxes and Feature Pyramid Networks (FPN).5 In the context of vehicle detection and pollution monitoring, YOLOv8s has demonstrated robust performance, achieving a precision of 0.936 and a mean Average Precision (mAP50) of 0.930 for detecting CNG automobiles.4 The consistent emphasis on YOLO's real-time capability and speed underscores a critical design principle for video analytics systems. For practical applications like traffic monitoring or environmental surveillance, where immediate action may be required, the ability to process video feeds with minimal latency is not merely a desirable feature but a fundamental requirement. This positions models like YOLO as a necessary architectural choice to meet the operational demands of continuous, live monitoring.2. Two-Stage Detectors (Faster R-CNN)In contrast to single-stage detectors, two-stage detectors, exemplified by Region-Based CNNs (R-CNN, Faster R-CNN), operate by first generating a set of "region proposals"—candidate object locations—and then classifying and refining these proposed regions in a subsequent stage.5 Faster R-CNN introduced a significant innovation by incorporating a Region Proposal Network (RPN) that shares convolutional features with the main detection network, thereby enabling "nearly cost-free region proposals".7 This design improved efficiency compared to its predecessors.While two-stage detectors generally offer higher localization accuracy, they typically incur a computational overhead that makes them slower than their single-shot counterparts.7 For instance, Faster R-CNN's complex two-stage architecture can lead to slower inference times, which may hinder its suitability for applications demanding strict real-time performance.25The following table provides a comparative overview of these object detection architectures based on available performance data:Model NameArchitecture TypeSpeed (ms/FPS)Accuracy (mAP)Suitability for Real-Time VideoYOLOv3Single-Stage22 ms (320x320)28.2 mAPHigh (3x faster than SSD)SSDSingle-StageSlower than YOLOv3Similar to YOLOv3ModerateFaster R-CNNTwo-StageTowards real-time (with RPN)Not specified for direct comparisonModerate to High (aims for real-time)Note: Specific mAP values for Faster R-CNN were not provided for direct comparison with YOLOv3 and SSD in the available material, but its design goal is real-time performance through RPN innovation.7This comparison highlights that for applications like smoky vehicle detection in live video, where rapid inference is paramount, single-stage detectors like YOLO are often preferred. While Faster R-CNN aims for real-time, YOLO's demonstrated speed advantage makes it a more compelling choice when immediate processing is critical.B. Semantic Segmentation for Fine-Grained AnalysisSemantic segmentation is a computer vision technique that goes beyond object detection by assigning a specific class label or category to every pixel in an image. This process generates a segmentation mask, providing a precise classification and detailed delineation of object boundaries.6 For phenomena like smoke, which lack rigid shapes, this pixel-level detail can offer more nuanced information than bounding boxes.1. Overview of Semantic Segmentation (U-Net, DeepLab)Common deep learning methods employed for semantic segmentation include Fully Convolutional Networks (FCN), U-Net, Pyramid Scene Parsing Network (PSPNet), and DeepLab.6 While models like U-Net and DeepLab can be adapted for object detection, their primary design is optimized for segmentation tasks, excelling at pixel-level classification and boundary definition.252. Hybrid Approaches for Smoke (e.g., DB-Net, SmokeSeger)For the specific challenge of vehicle smoke detection, semantic segmentation offers a more granular level of information compared to bounding-box-based object detection, providing precise location and detailed depiction of smoke boundaries.17 This level of detail is particularly valuable for analyzing amorphous and transient phenomena like smoke, enabling insights into smoke volume, dispersion direction, and source location.23 However, traditional semantic segmentation models present significant challenges: they are generally time-consuming and computationally intensive, and they demand extensive, high-quality labeled data, which is often scarce for niche applications like vehicle smoke.17To address these limitations, hybrid approaches have emerged that seek to balance localization accuracy with computational speed. The Deep Block Network (DB-Net), for instance, proposes a block-wise recognition strategy that positions it between traditional object detection and full semantic segmentation in terms of granularity. DB-Net employs a dual-branch architecture, consisting of a main branch for feature extraction and an aggregation branch for feature enhancement, to achieve this balance. This design allows it to operate with fewer parameters and achieve faster inference speeds than typical semantic segmentation models, while maintaining comparable or even superior performance.17 Similarly, SmokeSeger is another dual-branch model that couples a transformer branch with a CNN branch to enhance both global and local feature representation for urban scene smoke segmentation.23The development of these hybrid models directly addresses the inherent trade-off between the granularity of detection and computational performance. For a transient and amorphous phenomenon like smoke, precise pixel-level localization is highly desirable for accurate analysis. However, achieving this with traditional semantic segmentation models is computationally prohibitive and data-intensive. The emergence of hybrid models like DB-Net and SmokeSeger represents an intelligent engineering solution to this dilemma, allowing for some of the benefits of pixel-level detail without fully incurring the computational cost, thereby making such systems more viable for real-time video analysis. This suggests that a purely bounding-box approach might be insufficient for fine-grained analysis of smoky vehicles, necessitating these more sophisticated hybrid strategies.C. Leveraging Temporal Information in VideosVideo object detection fundamentally differs from image-based detection by incorporating temporal information to track objects across sequential frames.8 This temporal dimension is critical for accurately detecting and analyzing dynamic phenomena like smoke, which evolve and disperse over time.31. 3D Convolutional Neural Networks (3D CNNs)3D CNNs are specifically designed to process video data by applying convolutions across three dimensions: height, width, and time. This architecture allows them to proficiently extract spatial features from individual frames while simultaneously capturing temporal dependencies across successive frames.8 By processing volumetric data, 3D CNNs are capable of learning spatio-temporal features that represent short-term motion patterns and dynamic changes within video sequences.28 For tasks such as black smoke detection, 3D convolution modules are instrumental in capturing these short-term dynamic and semantic features from consecutive video frames, providing a richer representation than what 2D CNNs can offer from single frames.32. Recurrent Neural Networks (RNNs) and LSTMsRecurrent Neural Networks (RNNs), particularly Long Short-Term Memory (LSTM) networks, are vital components in video object detection due to their inherent ability to model sequential data and capture long-range dependencies over time.8 LSTMs are equipped with memory cells and gates that allow them to remember information from previous frames and utilize it to process the current frame, effectively combining image information over extended periods.1 This capability is particularly beneficial for improving the accuracy of smoke vehicle detection, where the temporal evolution of smoke plumes provides crucial cues.1 Bi-directional LSTMs (Bi-LSTMs) further enhance this by processing data in both forward and backward directions, leading to a more comprehensive understanding of the entire sequence.29 A powerful approach for dynamic recognition tasks involves combining 3D-CNNs with LSTMs. In this hybrid architecture, the 3D-CNN extracts robust spatial features and short-term temporal cues, while the LSTM network then learns the long-term temporal dependencies from the sequence of features generated by the 3D-CNN.283. Attention Mechanisms for Spatio-Temporal FocusAttention mechanisms have become a cornerstone in deep learning, enabling models to selectively focus on the most relevant parts of the input data. In the context of video analysis, this includes self-attention and multi-head attention, which allow models to concentrate on critical spatio-temporal regions and discern relationships between elements within a sequence.29 For smoke detection, attention mechanisms can significantly enhance the model's ability to identify black smoke while simultaneously minimizing the adverse impact of visual noise, such as occlusions and shadows.3 These mechanisms are pivotal in improving feature learning and directing the model's focus to crucial details pertinent to motion detection and the subtle characteristics of smoke.29 Furthermore, channel-wise attention mechanisms have been shown to yield substantial accuracy improvements in fire and smoke detection tasks.314. Integrating Object Tracking for ConsistencyEffective video object detection often involves combining frame-level object detection with robust tracking techniques. These techniques, such as optical flow and motion estimation, are employed to maintain temporal information and predict the positions of objects across successive frames.7 This integration is crucial for achieving temporal consistency in detections, which is particularly important for dynamic objects like smoke plumes and can significantly reduce the "flickering" effect often observed in frame-by-frame detections.33 For instance, combining a YOLO detector with a tracker like DeepSORT can provide a comprehensive system for real-time monitoring and improved object tracking over time.12The transient nature of smoke means its presence and characteristics change rapidly over time. Relying solely on frame-by-frame detection, as a 2D CNN would, inevitably leads to inconsistent or "flickery" results.33 The need to detect smoky vehicles from "videos" inherently demands the capture of temporal information. The effective use of 3D CNNs for short-term temporal features, LSTMs for long-term dependencies, and attention mechanisms for focusing on critical spatio-temporal regions collectively highlights that a single approach to temporal modeling is insufficient.1 Successful smoky vehicle detection requires a multi-modal temporal strategy that integrates various mechanisms to capture the full dynamic signature of smoke, from its initial appearance to its dispersion, ensuring robust and consistent detection across video sequences. This also implies that simple object detection models alone may not suffice and often require augmentation with tracking capabilities or more complex temporal architectures to achieve reliable performance.III. Step-by-Step Implementation GuideA. Step 1: Data Acquisition and AnnotationThe foundation of any successful deep learning model lies in the quality and quantity of its training data. For smoky vehicle detection, this necessitates high-quality, diverse, and meticulously annotated video data.241. Sourcing Relevant Video DataCollecting relevant video data is the initial critical step. This involves acquiring real-world surveillance videos from various traffic scenes that capture vehicles emitting smoke.17 Specialized datasets have been developed to address this need. The LaSSoV (Large-Scale Smoky Vehicle) dataset, for example, comprises 75,000 annotated smoky vehicle images, while its companion, LaSSoV-video, includes 163 long videos with segment-level annotations, indicating the start and end frames of smoke events.1 Another valuable resource is the Polygon-based annotated Vehicle Smoke Segmentation dataset (PoVSSeg), which provides highly precise pixel-level polygon annotations for vehicle smoke, crucial for fine-grained analysis.172. The "Cold Boot Process" for Dataset CreationManually collecting a large-scale dataset of smoky vehicles from scratch is exceptionally challenging due to the infrequent occurrence of smoky frames in typical surveillance footage.22 To overcome this data scarcity, researchers have developed an efficient "cold boot process" for dataset creation:
Initial Collection and Annotation: This phase begins with a targeted effort to generate initial data. Researchers might rent smoky vehicles and drive them within the view of various surveillance cameras. All video frames containing smoke are then captured, and a small subset (e.g., approximately 1000 frames) is manually annotated with bounding boxes around the smoke regions using annotation tools like LabelMe.22
Rough Detector Training: The small, manually annotated dataset is then used to train a preliminary, lightweight deep learning detector, such as YOLOv5s. This initial model, though not highly accurate, serves as a basic smoke identifier.22
Automated Collection and Human Verification: The roughly trained detector is deployed in surveillance cameras. It automatically gathers thousands of potential smoky frames by flagging any frame where smoke is detected above a certain threshold (e.g., 0.05).22 This large volume of automatically collected frames is then subjected to meticulous human verification. Trained annotators carefully review each frame to confirm the presence of actual smoke from vehicles and accurately annotate the bounding boxes. This iterative process allows for efficient scaling of data collection while maintaining high quality.22
This multi-stage approach addresses the fundamental challenge of limited annotated data. By leveraging a small initial human effort to bootstrap an automated collection process, which is then refined by further human verification, the "cold boot process" provides a pragmatic solution to data scarcity in niche computer vision tasks. This ensures both the quantity and quality of the training data, which are paramount for the robust performance of deep learning models.3. Detailed Annotation GuidelinesAnnotations are the metadata applied to video elements, serving as the ground truth for training machine learning algorithms to interpret dynamic real-world scenarios.34 For smoky vehicle detection, this involves precisely marking the smoke region and, typically, the vehicle itself.Several types of annotations can be employed:
Bounding Boxes: These provide rectangular coordinates that define the approximate location of an object. They are commonly used in object detection datasets, such as LaSSoV, for rough localization.17
Polygons/Segmentation Masks: These offer pixel-level accuracy by outlining the precise boundaries of an object. For amorphous objects like smoke, which lack clear edges, segmentation masks are crucial for detailed depiction and fine-grained analysis, as seen in the PoVSSeg dataset.17
Segment-level Annotations: For video datasets, this involves annotating the temporal segments of long videos, indicating the initial and final frames where smoke is present.22
When performing annotations, it is imperative to use robust and user-friendly software tools. LabelMe, for instance, has been utilized for both the LaSSoV and PoVSSeg datasets.17 Adhering to best practices is essential: maintaining consistency in the level of detail, choice of labels, and annotation types across the entire dataset is critical. Striving for the highest level of accuracy is paramount, as precise annotations directly lead to better-performing AI models. For complex objects or scenes, using multiple annotations can dramatically improve the quality of the training data. Regular review and error checking of annotations are also hallmarks of high-quality annotation work.344. Essential Data Augmentation TechniquesData augmentation is a vital strategy in deep learning, particularly for tasks where training data might be limited or lack diversity. It involves artificially expanding the training dataset by creating modified versions of existing images or video frames. This process is crucial for increasing the diversity of the training data, enhancing the model's generalization capabilities, and mitigating the effects of data scarcity.31 This is especially pertinent for smoke detection, given the high variability in smoke's appearance.Common data augmentation techniques include:
Geometric Transformations: Such as rotation, scaling (zooming in/out), flipping (horizontal/vertical), and translation (shifting the image).31 These help the model learn to recognize objects regardless of their orientation or size in the frame.
Color Space Transformations: Including color jittering, which involves varying brightness, saturation, or contrast.36 This makes the model robust to different lighting conditions.
Noise Injection: Adding random noise to images to simulate real-world sensor imperfections or environmental disturbances.31
Applying structured data augmentation can broaden the training sample diversity, supporting consistent performance across varied environmental scenarios.31 For datasets with imbalanced classes, where smoky vehicles are a minority class, augmentation can be targeted. For example, duplicating or replicating bounding boxes of minority classes can increase their representation in the training data, helping the model learn from more examples of the less frequent class.38The variability of smoke's appearance due to factors like color, texture, shape, density, and lighting is a significant challenge.1 Data augmentation techniques are not merely about increasing dataset size; they serve as a powerful proxy for simulating the vast range of real-world conditions—varying angles, distances, lighting, and atmospheric effects—that the model will encounter. This approach directly addresses the challenge of limited generalization capabilities 39 and the decreased accuracy observed when environmental conditions change.10 By exposing the model to a wider spectrum of variations it hasn't explicitly seen in the raw training data, data augmentation enables the model to become more robust and perform reliably in unpredictable real-world environments.B. Step 2: Model Selection and Transfer LearningThe choice of deep learning model and the application of transfer learning are pivotal decisions that significantly influence the success of a smoky vehicle detection system.1. Choosing the Right Base ModelFor real-time smoky vehicle detection, the selection of a model that optimally balances speed and accuracy is paramount. YOLO (You Only Look Once) and its variants, such as YOLOv5, YOLOv8, and YOLOv11, stand out as state-of-the-art architectures for real-time object detection.5 Their single-shot detection mechanism and efficient utilization of GPU resources make them particularly well-suited for high-throughput video analysis.5 YOLOv8, for instance, is highly recommended for its rapid inference times and competitive accuracy, making it a strong candidate for this application.4 Furthermore, combining YOLOv5 with tracking algorithms like DeepSORT has proven effective for real-time monitoring and tracking of objects over time.12The emphasis on "real-time" capability, as consistently highlighted for YOLO models, is not merely a desirable feature but a stringent operational constraint for practical applications. In contexts like traffic monitoring or environmental surveillance, where immediate action might be required, the ability to process video feeds with minimal latency is critical.40 This means that while more complex models might offer marginal gains in accuracy, they are often unsuitable if they cannot meet the stringent latency requirements of such systems. Therefore, the selection of a YOLO-based model is not just a suggestion but a strategic decision dictated by the operational environment and the need for timely, actionable intelligence.2. The Power of Transfer Learning and Fine-tuningTransfer Learning (TL) is a highly effective strategy for boosting the performance of object detectors, especially when faced with limited domain-specific datasets, a common scenario for specialized tasks like smoky vehicle detection.24 TL involves a two-stage process:
Initial Training (Pre-training): A deep learning model is first trained from scratch on a very large and diverse source dataset (e.g., ImageNet, COCO) for a general computer vision task like image classification or generic object detection.42 This allows the model to learn robust, low-level to mid-level visual features (e.g., edges, textures, basic shapes) that are broadly applicable across many visual domains.
Fine-Tuning: The pre-trained model's learned weights and filters are then used as an initialization point for training on a much smaller, target-specific dataset (e.g., smoky vehicle videos). During fine-tuning, the learning rates are typically set much lower than in initial training to preserve the valuable pre-learned features, and some initial layers of the network may even be frozen to prevent catastrophic forgetting.42
The benefits of transfer learning are substantial: it significantly enhances detection precision (e.g., achieving up to 79.2% mean Average Precision (mAP@0.5) for wildfire smoke detection), drastically reduces the required training time, and markedly increases the model's generalizability to new, unseen data within the target domain.42 Furthermore, an iterative transfer learning strategy, where the model is continuously refined based on user feedback from the production environment, can further enhance its adaptability and accuracy over time.43Transfer learning directly addresses the persistent challenge of "limited annotated data" for smoke detection.1 By leveraging the "rich prior knowledge embedded in extensively pre-trained 2D foundation models" 46, the need for a massive, custom-annotated dataset for smoky vehicles from scratch is greatly reduced. This means that instead of undertaking the prohibitive task of building a dataset large enough to train a complex deep learning model from zero, one can start with a model that already possesses a sophisticated understanding of generic visual features. This approach drastically minimizes the time, cost, and effort associated with data collection and annotation, making the development of a high-performing smoky vehicle detection system significantly more feasible.3. Utilizing Pre-trained WeightsThe availability of pre-trained weights, derived from models trained on vast and diverse datasets, is an invaluable asset in the transfer learning paradigm. These weights encapsulate a generalized understanding of visual patterns, serving as an excellent starting point for specialized tasks.Key pre-trained datasets commonly utilized include:
Microsoft Common Objects in Context (COCO): A heterogeneous source dataset widely used for general object detection tasks, containing a broad range of common objects.42
Flame and Smoke Detection Dataset (FASDD): A more homogeneous source dataset specifically curated for fire and smoke detection, comprising over 95,000 images. While it includes diverse smoke scenarios (e.g., cigarettes, burning buildings), it provides a strong foundation for smoke-related feature learning.42
ImageNet: A foundational dataset primarily used for pre-training classification backbones, enabling models to learn robust image features.28
Frameworks like TorchVision offer readily available pre-trained weights for various architectures, simplifying their integration into custom projects.44 It is crucial to note that when utilizing pre-trained models, proper preprocessing of input images is essential. This includes resizing to the correct resolution, applying appropriate inference transforms, and rescaling pixel values, as specified by the documentation accompanying the pre-trained weights. Failure to adhere to these preprocessing guidelines can lead to a significant decrease in model accuracy or incorrect outputs.44C. Step 3: Model Training and OptimizationOnce data is prepared and a base model is selected, the next critical phase involves training and optimizing the deep learning model.1. Selecting a Deep Learning FrameworkThe choice of deep learning framework significantly impacts the development workflow, flexibility, and deployment capabilities. The two predominant frameworks in the field are PyTorch and TensorFlow.47 Both frameworks provide high-level programming interfaces and leverage GPU-accelerated libraries for efficient computation.47
PyTorch: This open-source library is favored for its user-friendly Pythonic interface, which simplifies debugging and customization. Its dynamic computation graph offers greater flexibility, making it particularly advantageous for research and rapid prototyping.47 PyTorch supports easy execution with native Python code and asynchronous operations, contributing to a more fluid development experience.47
TensorFlow: Developed by Google, TensorFlow is a powerful and versatile open-source framework for machine learning and numerical computations. While it historically relied on static computational graphs, its eager execution mode provides a dynamic development experience akin to PyTorch. TensorFlow is well-suited for efficient production deployment, particularly on mobile and embedded devices, and offers extensive toolkits for large-scale and distributed training.48 Keras, a high-level API, can be used on top of TensorFlow, offering a more beginner-friendly interface.47
The optimal choice between these frameworks often depends on specific project requirements: PyTorch is generally preferred for its flexibility in research and experimentation, while TensorFlow (especially with its production-oriented features) is often chosen for large-scale deployments and robust scalability. Keras, due to its simplicity, is an excellent starting point for those new to deep learning.472. Data Loading and Preprocessing PipelinesData preprocessing is a fundamental and crucial step in any machine learning pipeline, ensuring that data is in a suitable format for model consumption and optimizing training efficiency.35In PyTorch, the torchvision.transforms module provides a rich set of predefined transformations for image preprocessing, such as converting images to tensors and normalizing pixel values.35 The torch.utils.data.DataLoader class is an indispensable tool for efficiently loading and preprocessing data in batches, which significantly optimizes memory usage and training performance.35 Best practices in PyTorch data preprocessing include leveraging GPU acceleration for faster operations, implementing custom transformation functions for unique dataset requirements, and ensuring proper data normalization to prevent features with larger scales from dominating the learning process.35For video data, specialized approaches are necessary to handle the temporal dimension. In TensorFlow/Keras, classes like FrameGenerator are designed to yield tensor representations of video frames along with their associated labels. This approach is memory-efficient as it avoids loading the entire video sequence into memory simultaneously.50 Common preprocessing steps for video include downsampling and resizing video frames to a consistent input dimension for the model.50Beyond basic preprocessing, data augmentation techniques are critical for enhancing model robustness. As discussed previously, applying a variety of augmentations such as rotation, translation, scaling, cropping, flipping, and color jittering helps increase the diversity of the training data, enabling the model to generalize better to real-world variations.363. Choosing and Customizing Loss FunctionsLoss functions are mathematical constructs that quantify the discrepancy between a model's predicted outputs and the actual ground truth labels. Their primary role is to guide the optimization process during training, enabling the model to learn and minimize errors.51 For object detection tasks, metrics like Intersection over Union (IoU), which measures the overlap between predicted and ground truth bounding boxes, are often integrated into the loss calculations to encourage accurate localization.52A significant challenge in smoky vehicle detection is the inherent class imbalance within datasets. Typically, non-smoky vehicles vastly outnumber smoky ones. This imbalance can lead to skewed detectors that favor the more commonly occurring "non-smoky" class, potentially at the expense of accurately detecting the rarer "smoky" instances.54 To mitigate this, several strategies can be employed:
Weighted Loss: This involves assigning different weights to the components of the loss function based on the frequency of each class. By giving a higher weight to the minority class (smoky vehicles), its contribution to the total loss is increased, forcing the model to pay more attention to these crucial examples.38 Focal Loss, a specialized loss function, is particularly relevant for dense object detection and effectively addresses class imbalance by down-weighting the loss contribution of well-classified examples, thereby focusing training on hard-to-classify instances.38
IoU Thresholding: For the majority class, a higher Intersection over Union (IoU) threshold can be set for positive detections. This ensures that only the most accurate predictions for the abundant class are considered positive, reducing its dominance in the training process.38
Class-Specific Sampling: During the creation of mini-batches for training, strategies can be implemented to ensure a more balanced representation of different classes, regardless of their overall frequency in the dataset.38
The "amorphous" and "transparent" nature of smoke presents additional complexities for loss function design.17 Segmenting transparent structures is challenging because their appearance can change dramatically depending on the background, viewing angles, lighting conditions, and distortions.19 While standard loss functions like cross-entropy are commonly used for semantic segmentation 17, specialized losses such as Dice Loss 51 or custom loss structures that encourage spatial coherence and better handle ambiguous boundaries may prove beneficial for accurately segmenting transparent smoke. The DB-Net model, for instance, utilizes a standard cross-entropy loss within its block-wise prediction framework.17The selection and customization of loss functions are not generic choices but must be meticulously tailored to the specific visual characteristics of smoke and the statistical properties of the dataset. The need for weighted losses and Focal Loss for class imbalance, alongside considerations for Dice Loss in segmentation, indicates that the mathematical formulation of the loss function must align with the perceptual and statistical challenges posed by the target phenomenon. This emphasizes a deeper understanding of how the loss function influences the model's ability to learn and differentiate subtle, transient visual cues.4. Optimizers and Learning Rate SchedulersOptimizers are algorithms that govern how the parameters (weights and biases) of a deep learning model are adjusted during training to minimize the chosen loss function.56 They are the engines that drive the learning process.Common choices for optimizers include:
Gradient Descent (GD), Stochastic Gradient Descent (SGD), and Mini-Batch Gradient Descent: These are fundamental optimization methods. SGD with Momentum, which incorporates a fraction of the previous update to accelerate convergence, often demonstrates good generalization performance.56
Adaptive Optimizers (Adagrad, RMSprop, Adam, AdamW): These optimizers automatically adjust the learning rates for each individual parameter based on their historical gradients, often leading to faster and more stable convergence.56 Adam and its variant, AdamW (which decouples weight decay), are widely adopted and frequently perform well "out of the box" across various deep learning tasks.57
The learning rate (LR) is a critical hyperparameter that determines the step size taken during each iteration of parameter updates.56 An excessively high learning rate can cause instability during training, leading to oscillations around the optimal solution or even divergence, where the loss increases indefinitely.59 Conversely, a very low learning rate can significantly slow down convergence, making the training process inefficient.59Learning Rate Schedulers are algorithms that dynamically adjust the learning rate during the training process. Their purpose is to improve the optimizer's performance, facilitate more efficient convergence, and help the model avoid getting stuck in suboptimal local minima.37 Popular scheduling strategies include:
Step Decay: Reduces the learning rate by a fixed factor at predefined epochs or after a certain number of iterations.37
Exponential Decay: Decreases the learning rate exponentially over time, providing a smoother reduction.37
Cyclical Learning Rate Policies: Systematically vary the learning rate between a minimum and maximum value over training iterations.37 A common practice is to start with a relatively higher learning rate and gradually decay it as training progresses.58
Best practices for training involve continuously monitoring loss curves and validation accuracy to identify issues like overfitting or underfitting.37 Experimenting with different optimizers and learning rate schedulers is often necessary to find the optimal configuration for a specific task and dataset. Additionally, techniques like gradient clipping can be employed to prevent the problem of exploding gradients, which can destabilize training.37The training process for deep learning models, especially with complex video data, is highly sensitive to these optimization parameters. The observation that inappropriate learning rates can lead to "oscillations or divergence" 59, and that "schedulers" are crucial for "improving convergence and performance" 56, highlights the dynamic and adaptive nature of the optimization landscape. It is not sufficient to simply select a single optimizer and a fixed learning rate; a sophisticated training pipeline involves dynamically adjusting these parameters throughout the training process. This ensures stability, accelerates convergence, and ultimately leads to optimal model performance, underscoring that effective training is an adaptive, rather than static, process.D. Step 4: Model Evaluation and RefinementAfter training, a rigorous evaluation phase is essential to assess the model's performance, identify areas for improvement, and ensure its readiness for deployment. Performance metrics provide quantitative measures to evaluate how accurately the algorithm detects, locates, and classifies objects.521. Standard Object Detection MetricsThese metrics compare the model's predictions (which include the object class, bounding box coordinates, and a confidence score) against the ground truth annotations.52
Intersection over Union (IoU): This is an accuracy metric that quantifies the overlap between a predicted bounding box and its corresponding ground truth bounding box. A common threshold, such as IoU ≥ 0.5, is used to determine if a detection is considered a True Positive (TP).52
Precision: Precision measures the proportion of correctly identified positive predictions among all positive predictions made by the model. It focuses on minimizing false positives (FP) and is calculated as TP / (TP + FP).31 Prioritizing precision is crucial if false alarms are unacceptable in the application.52
Recall (Sensitivity): Recall measures the proportion of actual positive instances that were correctly identified by the model. It emphasizes finding all ground truth bounding boxes and minimizing false negatives (FN). It is calculated as TP / (TP + FN).31 Prioritizing recall is important if missing detections is unacceptable.52
F1 Score: The F1 score is the harmonic mean of precision and recall, providing a single metric that balances both. It is particularly useful when a balanced trade-off between false positives and false negatives is desired.31
Average Precision (AP) & Mean Average Precision (mAP): AP integrates precision, recall, and the model's confidence for a single object class. Mean Average Precision (mAP) extends this by averaging the AP across all object classes and often considers various IoU thresholds (e.g., mAP@0.5, which uses an IoU threshold of 0.5, or mAP@0.5:0.95, which averages AP across IoU thresholds from 0.5 to 0.95).16 mAP is widely regarded as the primary metric for evaluating overall detection accuracy.16
2. Video-Specific Metrics for Temporal ConsistencyWhile standard object detection metrics evaluate performance on individual frames, video-based applications necessitate metrics that account for temporal consistency and tracking accuracy. A high mAP on individual frames does not guarantee smooth, consistent detections across a video sequence, which can lead to "flickery" or unstable results.33
Multi-Object Tracking Accuracy (MOTA): This metric incorporates identity switches (IDSW), which occur when a single ground truth object is assigned to different predicted tracks over time. MOTA penalizes these track assignment changes between consecutive frames, providing an overall measure of tracking performance. However, it can sometimes be oversimplified in complex scenarios.60
Identification F1 (IDF1): IDF1 addresses some of MOTA's limitations by focusing on how long a tracker correctly identifies an object's identity. It is based on Identification Precision (IDP) and Identification Recall (IDR) and computes assignments across the entire video sequence, making it sensitive to long-term tracking consistency.60
Higher Order Tracking Accuracy (HOTA): HOTA is a more recent and comprehensive metric designed to provide a balanced assessment of both detection and association performance, addressing the shortcomings of MOTA and IDF1. It incorporates localization accuracy and uses global alignment to evaluate how well objects are detected and their identities maintained over time.60 HOTA has emerged as the standard metric for benchmarking tracking algorithms.60
Association Accuracy (AssA): As a component of HOTA, AssA specifically evaluates how accurately a tracker maintains object identities across frames, focusing on the temporal consistency of ID assignments.60
For real-world video surveillance, the perceived quality and practical utility of the system depend not just on whether a smoky vehicle is detected in a single frame, but on how consistently it is detected and tracked across multiple frames. This temporal coherence is critical for downstream applications such as vehicle counting, pollution monitoring, or anomaly detection. The use of video-specific metrics moves the evaluation beyond static image analysis to a comprehensive assessment of dynamic video performance.3. Assessing Model Robustness and GeneralizationA robust smoky vehicle detection system must perform reliably across a wide range of real-world conditions. Therefore, evaluating model performance in challenging low-visibility environments, such as those affected by smoke, fog, rain, low-light, glare, or backlighting, is critical.21 Models must be capable of accurately distinguishing vehicle smoke from visually similar non-smoke phenomena like clouds or fog.16Robustness metrics are quantitative measures that assess the stability of a model's performance across different visibility conditions.21 This includes evaluating the model's sensitivity to specific degradations, such as motion blur, which can significantly impact detection accuracy in dynamic video streams.21 Furthermore, adversarial robustness procedures, such as WARP (Wildfire Adversarial Robustness Procedure), can be employed to evaluate how well models perform when subjected to random perturbations or noise. This is particularly relevant given the subtle and often ambiguous nature of smoke, where minor visual changes can lead to misclassifications.16The challenges section highlighted the inherent variability of smoke and its potential confusion with other visual artifacts.1 The concept of "adversarial robustness" and the need to "distinguish smoke from similar objects" 16 suggest that the environment itself can present visual cues that might "trick" the model. Therefore, evaluating a model's resilience against these "natural adversarial examples"—ranging from subtle smoke plumes to benign smoke-like phenomena—is crucial for ensuring real-world reliability. False alarms, especially in environmental monitoring or traffic management, can be costly and undermine trust in the system.E. Step 5: Model Deployment and IntegrationThe final stage involves deploying the trained and evaluated model into a live environment and integrating it seamlessly with existing infrastructure.1. Optimizing Models for InferenceTo ensure real-time performance and efficient resource utilization, especially in video processing, AI model optimization is crucial. This process focuses on making models faster, smaller, and more accurate without compromising their core task performance.45Key optimization techniques include:
Pruning: This involves removing unnecessary parameters (e.g., connections, neurons, or even entire layers) from the neural network. Pruning reduces the model's size and enables more efficient inference by decreasing computational load.45 It can be applied during the training phase (train-time pruning) or after the model has been fully trained (post-training pruning).63
Quantization: This technique reduces the numerical precision of the weights and activations within the neural network. For instance, converting 32-bit floating-point numbers to lower precision formats like 8-bit integers can significantly shrink model size (by 75% or more) and reduce memory and processing power requirements.45
ONNX Export: The Open Neural Network Exchange (ONNX) Runtime provides a standardized format and runtime for deep learning models. Exporting a model to ONNX allows for optimized inference across different hardware and frameworks, ensuring compatibility and efficiency regardless of where the model was initially trained.45
Models like YOLOv5n are notable for their efficiency, demonstrating powerful performance even when hardware acceleration is limited, processing images nearly twice as fast as newer, larger counterparts like YOLOv11n.42 This highlights the importance of selecting not just accurate models, but also those amenable to optimization for practical deployment.2. Edge vs. Cloud Deployment StrategiesThe choice between edge and cloud deployment, or a hybrid approach, is critical for real-time video surveillance systems.
Edge AI: In this strategy, data processing occurs directly on the device (e.g., surveillance cameras, drones) or on nearby edge computing nodes. This approach dramatically reduces latency by minimizing data transmission to a central server, enhances privacy by processing sensitive data locally, and reduces dependence on internet connectivity.9 Edge AI is ideal for real-time applications like traffic monitoring and autonomous vehicles, where immediate responses are critical.9
Cloud AI: Cloud-based deployments offer significant scalability on demand, providing access to vast computational resources for training complex deep learning models and processing large-scale datasets.64 They also facilitate seamless model updates and deployments.64 Cloud AI is well-suited for complex training tasks, data aggregation, and long-term trend analysis.64
For smoky vehicle detection, a hybrid cloud-edge collaborative architecture often proves most effective. Edge devices can handle real-time, low-latency detection of smoky vehicles, sending only metadata or alerts (e.g., bounding box coordinates, confidence scores, timestamps) to the cloud. The cloud can then be used for further analysis, long-term data storage, model retraining, and global system management.43 The need for real-time detection, coupled with the challenges of latency and bandwidth in video streaming, naturally leads to a distributed intelligence paradigm. Instead of all video data being sent to a central cloud for processing (which incurs significant latency and bandwidth costs), edge AI allows for faster processing with low latency directly on the device.64 For large-scale deployments of smoky vehicle detection across a city's traffic cameras, a hybrid edge-cloud architecture is not merely an option but a necessary design choice to ensure both immediate responsiveness and efficient centralized data management and model updates, leading to a truly scalable system.3. Integrating with Existing SystemsSeamless integration with existing video surveillance and traffic monitoring infrastructure is paramount for the practical adoption of a smoky vehicle detection system. This requires adherence to standard APIs and streaming protocols.
APIs (Application Programming Interfaces): APIs enable programmatic management, delivery, and interaction with video content.66 REST APIs and WebSocket APIs are commonly used to support real-time data exchange and integration with existing security platforms.68 Advanced APIs increasingly offer AI-powered video features, including object detection, as built-in functionalities.66
Streaming Protocols:

RTSP (Real-Time Streaming Protocol): This protocol defines how surveillance camera software transmits and controls media data. It offers reliable video playback and is widely supported by most IP cameras, making it a common choice for receiving live video feeds.69
ONVIF (Open Network Video Interface Forum): ONVIF is a global standard that ensures compatibility and interoperability between IP-based physical security products from different manufacturers. It provides specifications for video streaming, device configuration, and control, simplifying the integration of diverse camera systems.69


These protocols enable the detection system to efficiently receive live video feeds from a city's existing CCTV cameras.3 The integration should also support real-time analytics and monitoring, allowing for the tracking of playback performance, viewer engagement, and error rates via APIs and webhooks.664. Managing Latency for Real-time PerformanceLatency, defined as the delay between a user's input (or a system's observation) and the system's output (or response), is a critical factor in AI deployments. High latency can severely impact model performance and user experience, potentially leading to inaccurate results or delayed interventions.40 For example, a delay of just 100 milliseconds in an AI system can significantly reduce user engagement.40Sources of latency in deep learning systems include network latency (time for data to travel across a network), processing latency (time for the AI model to process input), queueing latency (time spent waiting in processing queues), and data access latency (time to retrieve data from storage).40Achieving "low-latency monitoring" means viewing live video footage with virtually no discernible delay between capture and display.41 Edge AI plays a crucial role in minimizing latency by processing data locally on devices or nearby nodes, preventing the need for data to travel back and forth to the cloud.64 This localized processing results in lightning-fast responses, which are often unattainable with traditional cloud-dependent systems.64Tools for latency analysis include continuous time monitoring, collection of high-quality data, and predictive analytics to forecast and mitigate potential delays.40 AI observability solutions, such as Coralogix's AI Center, provide real-time insights into system behavior, enabling proactive identification and management of latency issues.40The statement that even small delays can lead to significant operational costs, as demonstrated by Amazon's experience, elevates latency from a mere technical specification to a critical operational and business concern.40 For smoky vehicle detection, high latency translates directly into delayed environmental responses or inefficient traffic management, which can have substantial negative consequences. Therefore, minimizing latency is not just a performance goal but a paramount design objective that directly influences the system's practical utility and its ability to deliver timely, actionable intelligence.IV. Advanced Considerations and Future OutlookBeyond the core implementation steps, several advanced considerations are crucial for building a robust, reliable, and future-proof smoky vehicle detection system.A. Mitigating False AlarmsFalse alarms represent a significant challenge in smoke detection systems, primarily due to the visual similarities between actual smoke and various non-smoke phenomena (e.g., steam, dust, fog, shadows).10 This ambiguity can lead to misclassifications and undermine the system's credibility.Advanced methodologies are being developed to address this. One notable approach is Unbiased Multiple Instance Learning (UMIL). Traditional Multiple Instance Learning (MIL) methods can develop a "context bias," where the detector might associate normal smoke (e.g., from a chimney) with an anomaly because it frequently appears alongside true anomalies (e.g., an explosion).15 UMIL tackles this by learning unbiased anomaly features. It identifies invariant features across both confidently classified (normal/abnormal) and ambiguous sample groups, effectively removing these context-dependent biases. This prevents false alarms triggered by benign smoke snippets.15Another effective strategy, exemplified by the Coarse-to-fine Deep Smoky vehicle detection (CoDeS) framework, involves a multi-stage approach. CoDeS first employs a lightweight YOLO detector for rapid smoke detection with a high recall rate. Subsequently, it applies a smoke-vehicle matching strategy to specifically eliminate instances of non-vehicle smoke. The results are then further refined using a sophisticated 3D model that leverages spatial-temporal information.1 This cascaded framework helps to filter out irrelevant smoke sources, ensuring that only smoke originating from vehicles is flagged.The persistent issue of false positives, arising from smoke's visual ambiguity, underscores that merely detecting "smoke" is insufficient; the system must specifically detect "smoky vehicles." This necessitates the incorporation of contextual intelligence—the ability to differentiate smoke from a vehicle exhaust from other smoke sources or visual noise. Solutions like UMIL and smoke-vehicle matching are not just technical fixes but represent the model's capacity to learn and apply nuanced contextual rules, moving beyond simple visual pattern recognition to a more sophisticated understanding of the scene.B. Adapting to Varying Environmental ConditionsThe inherent variability of smoke and the dynamic nature of real-world environments demand that smoke detection models be exceptionally robust. They must perform consistently across diverse environmental conditions, including variations in smoke density, lighting conditions (e.g., day, night, glare), background complexity, and the presence of occlusions.3Effective solutions for enhancing adaptability include:
Comprehensive Data Augmentation: As discussed in Section III.A.4, this is fundamental for increasing dataset diversity and improving model generalizability. Techniques like rotation, scaling, flipping, color jittering, and noise injection simulate a wide range of real-world scenarios.31
Robust Feature Extraction: Deep learning models, particularly CNNs, can extract "depth features" that are inherently more adaptable to variable environments compared to handcrafted features.10 Architectures incorporating Gabor convolutional networks 1 or dual-branch structures, such as those in SmokeSeger 23 and for black smoke detection 3, are designed to enhance the representation of both global and local features, making them more resilient to visual variations.
Attention Mechanisms: Integrating attention mechanisms within the model architecture helps it dynamically focus on the most salient features of smoke while minimizing the influence of distracting noise and irrelevant background elements.3
Multi-modal Data Integration: While primarily explored for fire detection, the concept of incorporating multi-modal data (e.g., thermal or multispectral imagery) alongside visual spectrum data holds promise for more robust smoke detection in challenging conditions, especially when visual cues are ambiguous.24
The consistent emphasis on the variability of smoke and environmental conditions highlights that generalization is not a one-time problem to be solved during initial model training; it is a continuous engineering challenge. Real-world conditions will always present novel variations that the model has not explicitly encountered during training. This implies that a truly effective system must be designed with inherent adaptability, potentially through continuous learning mechanisms, active learning strategies, or a robust feedback loop from deployment to retraining, to ensure sustained performance over time.C. Continuous Learning and Model UpdatesThe dynamic nature of real-world environments and the evolving characteristics of vehicle emissions necessitate that a smoky vehicle detection system is not a static deployment but rather a continuously learning and adapting entity.The iterative transfer learning strategy, which incorporates user feedback from the production environment, provides a compelling model for this continuous improvement.43 This approach allows models to adapt to new data distributions, refine their accuracy, and enhance their robustness over time. It suggests a paradigm where models are not just trained once but are regularly updated with new, challenging data and insights derived from their performance in the field.Maintaining optimal model performance in such dynamic environments requires robust Machine Learning Operations (MLOps) practices. MLOps encompasses the processes and tools for managing the entire machine learning lifecycle, from data collection and model training to deployment, monitoring, and continuous improvement. It ensures that the system can continuously monitor its performance in the wild, identify new types of smoky vehicles or challenging environmental conditions, efficiently collect and annotate this new data, retrain the models, and seamlessly redeploy updated versions. This transforms a one-off project into a sustainable, high-performing system capable of adapting to evolving real-world scenarios and maintaining its effectiveness over the long term.V. ConclusionDetecting smoky vehicles from video footage using deep learning models offers a powerful, scalable, and cost-effective solution for environmental monitoring and traffic management. This report has detailed the comprehensive steps required for such an implementation, from data acquisition and model selection to training, evaluation, and deployment.The critical role of deep learning, particularly single-stage object detectors like YOLO and hybrid semantic segmentation approaches, is evident in their ability to handle the complexities of real-time video analysis and the amorphous nature of smoke. Leveraging temporal information through 3D CNNs, LSTMs, and attention mechanisms is essential for capturing the dynamic characteristics of smoke and ensuring consistent detection across video sequences.Practical implementation hinges on meticulous data acquisition and annotation, often requiring iterative "cold boot" processes to overcome data scarcity. The power of transfer learning and fine-tuning with pre-trained weights significantly mitigates the data annotation bottleneck, making the development more feasible. Robust training pipelines, incorporating optimized loss functions (especially for class imbalance), advanced optimizers, and dynamic learning rate schedulers, are crucial for achieving stable and high-performing models.Rigorous evaluation, extending beyond frame-level accuracy to include video-specific metrics for temporal consistency (e.g., HOTA), is vital for assessing real-world utility. Furthermore, models must be robust to varying environmental conditions, a continuous engineering challenge that necessitates ongoing adaptation. Finally, strategic deployment, whether at the edge, in the cloud, or a hybrid approach, alongside seamless integration with existing surveillance infrastructure via standard APIs and streaming protocols, ensures that the system delivers timely and actionable intelligence while managing critical factors like latency.While challenges such as smoke variability, potential false positives, and the need for continuous adaptation persist, the methodologies outlined provide a clear and actionable path to successful implementation. The future potential of such deep learning systems for real-time environmental monitoring, smart city initiatives, and proactive pollution control is immense, promising a significant positive impact on public health and ecological well-being.
